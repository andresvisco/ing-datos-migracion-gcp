# ğŸ¥ Proyecto MEDICUS: MigraciÃ³n IngenierÃ­a de Datos GCP

Este repositorio contiene el cÃ³digo fuente, la configuraciÃ³n y la documentaciÃ³n para la implementaciÃ³n de la plataforma de datos del proyecto **MEDICUS**, siguiendo una arquitectura de **Data Lakehouse** basada en el modelo de zonas **Bronze-Silver-Gold** sobre Google Cloud Platform (**GCP**).

El objetivo es ingestar, procesar y transformar datos de diversas fuentes de MEDICUS, asegurando calidad, trazabilidad y gobernanza para impulsar la analÃ­tica avanzada y la toma de decisiones.

---

## ğŸ—ºï¸ Arquitectura de Referencia

La arquitectura se basa en el **patrÃ³n Medallion** (Bronze, Silver, Gold) y estÃ¡ orquestada por **Composer (Apache Airflow)**, garantizando un flujo de datos robusto, escalable y mantenible.


---

## ğŸ› ï¸ TecnologÃ­as Clave (Stack TecnolÃ³gico)

El proyecto se despliega y opera exclusivamente en **Google Cloud Platform (GCP)**, aprovechando servicios gestionados para optimizar la eficiencia operativa.

| CategorÃ­a | Servicio / TecnologÃ­a | PropÃ³sito Principal |
| :--- | :--- | :--- |
| **OrquestaciÃ³n** | **Composer** (Apache Airflow) | GestiÃ³n, scheduling y monitoreo de los Data Pipelines (DAGs). |
| **Ingesta / RAW** | **Cloud Storage** (Bronze Zone) | Almacenamiento de datos crudos (RAW) en formatos **Parquet/Avro**. |
| **Procesamiento** | **Dataflow** / **Spark** | TransformaciÃ³n de datos ETL/ELT, limpieza y enriquecimiento. |
| **Data Warehouse** | **BigQuery** (Silver & Gold Zones) | Almacenamiento optimizado para el consumo, analÃ­tica y reporting. |
| **Calidad de Datos**| **Great Expectations** | ImplementaciÃ³n de validaciones de calidad de datos en la zona Bronze. |
| **Metadatos/CatÃ¡logo** | **Data Catalog** | Descubrimiento, inventario, linaje y gestiÃ³n de metadatos. |
| **Gobernanza** | **IAM, Cloud Logging, Versiones** | Seguridad, auditorÃ­a, monitoreo y control de versiones de datos. |

---

## ğŸ“‚ Estructura del Repositorio

La estructura de carpetas sigue las mejores prÃ¡cticas para un proyecto de Data Engineering y refleja las etapas del flujo de datos:
```plaintext
medicus-data-platform/
â”œâ”€â”€ dags/                          # Definiciones de pipelines para Composer (Airflow)
â”‚   â”œâ”€â”€ bronze_ingest_dag.py
â”‚   â”œâ”€â”€ silver_transform_dag.py
â”‚   â””â”€â”€ gold_analytics_dag.py
â”œâ”€â”€ dataflow_jobs/                 # Scripts de transformaciÃ³n para Dataflow
â”‚   â”œâ”€â”€ bronze_validation.py
â”‚   â”œâ”€â”€ silver_clean.py
â”‚   â””â”€â”€ gold_aggregation.py
â”œâ”€â”€ sql/                           # Consultas y scripts DDL/DML para BigQuery
â”‚   â”œâ”€â”€ silver_schemas/
â”‚   â””â”€â”€ gold_views/
â”œâ”€â”€ terraform/                     # ConfiguraciÃ³n de infraestructura como cÃ³digo (IaC)
â”‚   â””â”€â”€ gcp_resources.tf
â”œâ”€â”€ tests/                         # Pruebas unitarias, de integraciÃ³n y Great Expectations
â”‚   â””â”€â”€ ge_expectations.json
â”œâ”€â”€ docs/                          # DocumentaciÃ³n del proyecto (diagramas, decisiones de diseÃ±o)
â””â”€â”€ README.md            # DocumentaciÃ³n principal del proyecto
```

---

## ğŸ“¦ Fases del Flujo de Datos (Modelo Medallion)

### ğŸ¥‰ Bronze Raw Landing Zone

* **FunciÃ³n:** Ingesta de datos crudos *tal cual* (RAW) desde las fuentes (`MEDICUS Source`).
* **Proceso Clave:** **Dataflow** escribe el *raw data* en Cloud Storage (Parquet/Avro).
* **Calidad:** Se aplica **`Great Expectations`** para validar la *integridad bÃ¡sica* (ej. esquema, no-nulos) *antes* del procesamiento.
* **Salida:** Datos crudos y validados en **Cloud Storage (GCS)**.

### ğŸ¥ˆ Silver Procesado Curado

* **FunciÃ³n:** Limpieza, enriquecimiento y aplicaciÃ³n de lÃ³gica de negocio inicial.
* **Proceso Clave:** **Dataflow** realiza la **TransformaciÃ³n** (ETL/ELT) para curar y homogeneizar los datos.
* **Salida:** Tablas limpias y curadas en **BigQuery** (`Tablas silver`).
* **Calidad:** AplicaciÃ³n de chequeos de `Calidad Curado` post-transformaciÃ³n.

### ğŸ¥‡ Gold AnalÃ­tica y Reporting

* **FunciÃ³n:** Almacenamiento de datos listos para el consumo por usuarios de negocio, modelos de ML y herramientas de BI.
* **Proceso Clave:** Agregaciones, *joins*, creaciÃ³n de *features* y modelos de datos dimensionales/estrella en **BigQuery**.
* **Salida:** Tablas y vistas optimizadas para el rendimiento en **BigQuery** (`Tablas Gold`).
* **CatÃ¡logo:** Las tablas Gold son catalogadas en **Data Catalog** para su fÃ¡cil descubrimiento.

---

## ğŸ”’ Gobernanza y Control

Se han incorporado componentes de gobernanza esenciales:

* **Versionado de Datos:** ImplementaciÃ³n de estrategias de *versionado* a nivel de *schema* y *data* para la auditabilidad.
* **Metadatos y Linaje:** Uso de **Data Catalog** para trazar el linaje de los datos desde la fuente (Bronze) hasta el consumo (Gold).
* **Seguridad y AuditorÃ­a (IAM, PII):** AplicaciÃ³n de polÃ­ticas de **IAM** (Identity and Access Management) y controles de acceso basados en roles. Monitoreo constante a travÃ©s de **Cloud Logging**.
* **Logging & Monitoring:** Integrado en todas las zonas (Bronze, Silver, Gold) para asegurar la trazabilidad operativa de los pipelines.

---

## â–¶ï¸ GuÃ­a RÃ¡pida de Inicio

### Recursos del Entorno Dev

Los siguientes recursos estÃ¡n configurados para el entorno de desarrollo (dev), siguiendo la [GuÃ­a de Nomenclatura GCP](Documentacion/nomenclatura-gcp.md):

* **PROJECT_ID:** `medicus-data-dataml-dev`
* **BUCKET_BRONZE (GCS):** `medicus-data-bronze-raw-dev-uscentral1`
* **BigQuery Instance:** `medicus-data-dataml-dev`
* **Dataset Bronze:** `medicus-data-dataml-dev.medicus_bronze_raw_acumulado`
* **Formato de archivos Bronze:** `Parquet` (recomendado para optimizaciÃ³n de almacenamiento y consultas)
* **RegiÃ³n:** `us-central1`

### Pasos de ConfiguraciÃ³n

1.  **ConfiguraciÃ³n de Entorno:**
    * Instalar `gcloud CLI`, `terraform`, y `python v3.x`.
    * Autenticarse con `gcloud auth application-default login`.
    * Configurar proyecto: `gcloud config set project medicus-data-dataml-dev`
2.  **Despliegue de Infraestructura (Terraform):**
    ```bash
    cd terraform
    terraform init
    terraform plan -out=plan.tfplan
    terraform apply plan.tfplan
    ```
3.  **Despliegue de DAGs (Composer):**
    * Subir el contenido de la carpeta `dags/` al bucket de Composer.
4.  **EjecuciÃ³n:**
    * Activar el DAG principal (`bronze_ingest_dag`) desde la UI de Airflow.
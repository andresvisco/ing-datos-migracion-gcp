# üè• Proyecto MEDICUS: Migraci√≥n Ingenier√≠a de Datos GCP

Este repositorio contiene el c√≥digo fuente, la configuraci√≥n y la documentaci√≥n para la implementaci√≥n de la plataforma de datos del proyecto **MEDICUS**, siguiendo una arquitectura de **Data Lakehouse** basada en el modelo de zonas **Bronze-Silver-Gold** sobre Google Cloud Platform (**GCP**).

El objetivo es ingestar, procesar y transformar datos de diversas fuentes de MEDICUS, asegurando calidad, trazabilidad y gobernanza para impulsar la anal√≠tica avanzada y la toma de decisiones.

---

## üó∫Ô∏è Arquitectura de Referencia

La arquitectura se basa en el **patr√≥n Medallion** (Bronze, Silver, Gold) y est√° orquestada por **Composer (Apache Airflow)**, garantizando un flujo de datos robusto, escalable y mantenible.


---

## üõ†Ô∏è Tecnolog√≠as Clave (Stack Tecnol√≥gico)

El proyecto se despliega y opera exclusivamente en **Google Cloud Platform (GCP)**, aprovechando servicios gestionados para optimizar la eficiencia operativa.

| Categor√≠a | Servicio / Tecnolog√≠a | Prop√≥sito Principal |
| :--- | :--- | :--- |
| **Orquestaci√≥n** | **Composer** (Apache Airflow) | Gesti√≥n, scheduling y monitoreo de los Data Pipelines (DAGs). |
| **Ingesta / RAW** | **Cloud Storage** (Bronze Zone) | Almacenamiento de datos crudos (RAW) en formato **Parquet** (principal) y Avro. |
| **Procesamiento** | **Dataflow** / **Spark** | Transformaci√≥n de datos ETL/ELT, limpieza y enriquecimiento. |
| **Data Warehouse** | **BigQuery** (Silver & Gold Zones) | Almacenamiento optimizado para el consumo, anal√≠tica y reporting. |
| **Calidad de Datos**| **Great Expectations** | Implementaci√≥n de validaciones de calidad de datos en la zona Bronze. |
| **Metadatos/Cat√°logo** | **Data Catalog** | Descubrimiento, inventario, linaje y gesti√≥n de metadatos. |
| **Gobernanza** | **IAM, Cloud Logging, Versiones** | Seguridad, auditor√≠a, monitoreo y control de versiones de datos. |

---

## üìÇ Estructura del Repositorio

La estructura de carpetas sigue las mejores pr√°cticas para un proyecto de Data Engineering y refleja las etapas del flujo de datos:
```plaintext
medicus-data-platform/
‚îú‚îÄ‚îÄ dags/                          # Definiciones de pipelines para Composer (Airflow)
‚îÇ   ‚îú‚îÄ‚îÄ bronze_ingest_dag.py
‚îÇ   ‚îú‚îÄ‚îÄ silver_transform_dag.py
‚îÇ   ‚îî‚îÄ‚îÄ gold_analytics_dag.py
‚îú‚îÄ‚îÄ dataflow_jobs/                 # Scripts de transformaci√≥n para Dataflow
‚îÇ   ‚îú‚îÄ‚îÄ bronze_validation.py
‚îÇ   ‚îú‚îÄ‚îÄ silver_clean.py
‚îÇ   ‚îî‚îÄ‚îÄ gold_aggregation.py
‚îú‚îÄ‚îÄ sql/                           # Consultas y scripts DDL/DML para BigQuery
‚îÇ   ‚îú‚îÄ‚îÄ silver_schemas/
‚îÇ   ‚îî‚îÄ‚îÄ gold_views/
‚îú‚îÄ‚îÄ terraform/                     # Configuraci√≥n de infraestructura como c√≥digo (IaC)
‚îÇ   ‚îî‚îÄ‚îÄ gcp_resources.tf
‚îú‚îÄ‚îÄ tests/                         # Pruebas unitarias, de integraci√≥n y Great Expectations
‚îÇ   ‚îî‚îÄ‚îÄ ge_expectations.json
‚îú‚îÄ‚îÄ docs/                          # Documentaci√≥n del proyecto (diagramas, decisiones de dise√±o)
‚îî‚îÄ‚îÄ README.md            # Documentaci√≥n principal del proyecto
```

---

## üì¶ Fases del Flujo de Datos (Modelo Medallion)

### ü•â Bronze Raw Landing Zone

* **Funci√≥n:** Ingesta de datos crudos *tal cual* (RAW) desde las fuentes (`MEDICUS Source`), incluyendo exportaciones desde **QlikView**.
* **Proceso Clave:** **Dataflow** escribe el *raw data* en Cloud Storage en formato **Parquet** (formato principal, con soporte para Avro cuando sea necesario).
* **Formato Est√°ndar:** **Parquet** - Formato columnar optimizado que ofrece compresi√≥n eficiente, lectura selectiva y compatibilidad nativa con BigQuery y Dataflow.
* **Calidad:** Se aplica **`Great Expectations`** para validar la *integridad b√°sica* (ej. esquema, no-nulos, tipos de datos) *antes* del procesamiento.
* **Salida:** Datos crudos y validados en **Cloud Storage (GCS)** listos para transformaci√≥n a Silver.
* **Ejemplo DEV:** `gs://medicus-data-bronze-raw-dev-uscentral1/qlikview_exports/*.parquet`

### ü•à Silver Procesado Curado

* **Funci√≥n:** Limpieza, enriquecimiento y aplicaci√≥n de l√≥gica de negocio inicial.
* **Proceso Clave:** **Dataflow** realiza la **Transformaci√≥n** (ETL/ELT) para curar y homogeneizar los datos.
* **Salida:** Tablas limpias y curadas en **BigQuery** (`Tablas silver`).
* **Calidad:** Aplicaci√≥n de chequeos de `Calidad Curado` post-transformaci√≥n.

### ü•á Gold Anal√≠tica y Reporting

* **Funci√≥n:** Almacenamiento de datos listos para el consumo por usuarios de negocio, modelos de ML y herramientas de BI.
* **Proceso Clave:** Agregaciones, *joins*, creaci√≥n de *features* y modelos de datos dimensionales/estrella en **BigQuery**.
* **Salida:** Tablas y vistas optimizadas para el rendimiento en **BigQuery** (`Tablas Gold`).
* **Cat√°logo:** Las tablas Gold son catalogadas en **Data Catalog** para su f√°cil descubrimiento.

---

## üîí Gobernanza y Control

Se han incorporado componentes de gobernanza esenciales para cumplir con los **m√°ximos est√°ndares de calidad, trazabilidad y modularidad**:

* **Nomenclatura Estandarizada:** Convenciones rigurosas para nombres de recursos, datasets y buckets siguiendo la gu√≠a `nomenclatura-gcp.md`.
* **Etiquetado Obligatorio:** Todos los recursos GCP incluyen etiquetas (`labels`) para trazabilidad: `project`, `business_unit`, `environment`, `domain`, `managed_by`, `owner`.
* **Versionado de Datos:** Implementaci√≥n de estrategias de *versionado* a nivel de *schema* y *data* para la auditabilidad y reproducibilidad.
* **Metadatos y Linaje:** Uso de **Data Catalog** para trazar el linaje de los datos desde la fuente (Bronze) hasta el consumo (Gold), garantizando trazabilidad completa.
* **Seguridad y Auditor√≠a (IAM, PII):** Aplicaci√≥n de pol√≠ticas de **IAM** (Identity and Access Management) y controles de acceso basados en roles. Monitoreo constante a trav√©s de **Cloud Logging**.
* **Logging & Monitoring:** Integrado en todas las zonas (Bronze, Silver, Gold) para asegurar la trazabilidad operativa de los pipelines y facilitar la depuraci√≥n.
* **Modularidad:** Arquitectura basada en m√≥dulos Terraform reutilizables que garantizan consistencia y facilitan el mantenimiento.
* **Validaci√≥n Automatizada:** Scripts de validaci√≥n de nomenclatura (`validate_nomenclatura.py`) integrados en CI/CD para prevenir despliegues no conformes.

### üìã Est√°ndares de Calidad

Todos los recursos y procesos siguen estos principios:

1. **Consistency**: Nomenclatura uniforme en todos los entornos (dev, qa, prod).
2. **Traceability**: Linaje completo de datos desde ingesta hasta consumo.
3. **Security**: Principio de privilegio m√≠nimo y encriptaci√≥n end-to-end.
4. **Maintainability**: C√≥digo modular, documentado y versionado.
5. **Scalability**: Dise√±o que permite crecimiento horizontal sin refactorizaci√≥n mayor.

---

## üîß Configuraci√≥n del Entorno de Desarrollo (DEV)

El entorno de desarrollo (`dev`) se encuentra configurado con los siguientes par√°metros de infraestructura:

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| **PROJECT_ID** | `medicus-data-dataml-dev` | Proyecto GCP para el entorno de desarrollo |
| **BUCKET_BRONZE** | `medicus-data-bronze-raw-dev-uscentral1` | Bucket de almacenamiento para datos crudos (Bronze Zone) |
| **BigQuery Instance** | `medicus-data-dataml-dev` | Instancia de BigQuery para procesamiento y an√°lisis |
| **Dataset Bronze** | `medicus-data-dataml-dev.medicus_bronze_raw_acumulado` | Dataset acumulado de datos crudos en BigQuery |
| **Formato de Archivos Bronze** | **Parquet** | Archivos exportados desde QlikView en formato Parquet |

### üìä Flujo de Datos en DEV

Los datos en el entorno de desarrollo siguen este flujo:

1. **Exportaci√≥n desde QlikView**: Los datos se exportan desde QlikView en formato **Parquet** para optimizar el almacenamiento y rendimiento.
2. **Ingesta a Bronze Zone**: Los archivos Parquet se cargan al bucket `medicus-data-bronze-raw-dev-uscentral1`.
3. **Validaci√≥n de Calidad**: Se ejecutan validaciones con **Great Expectations** sobre los archivos Parquet.
4. **Carga a BigQuery Bronze**: Los datos validados se cargan al dataset `medicus_bronze_raw_acumulado` en el proyecto `medicus-data-dataml-dev`.
5. **Transformaci√≥n a Silver/Gold**: Los pipelines de Dataflow procesan los datos hacia las capas Silver y Gold seg√∫n las reglas de negocio.

### üéØ Formato Parquet: Ventajas

El uso de **Parquet** como formato est√°ndar en la zona Bronze ofrece:

- **Compresi√≥n eficiente**: Reducci√≥n del espacio de almacenamiento hasta 10x comparado con formatos de texto.
- **Lectura columnar**: Optimizaci√≥n de consultas que acceden solo a columnas espec√≠ficas.
- **Compatibilidad**: Integraci√≥n nativa con BigQuery, Dataflow, Spark y otras herramientas del ecosistema GCP.
- **Preservaci√≥n de tipos**: Mantenimiento de la integridad de los tipos de datos originales.
- **Metadatos embebidos**: Esquema y estad√≠sticas incluidos en el archivo para mejor catalogaci√≥n.

---

## ‚ñ∂Ô∏è Gu√≠a R√°pida de Inicio

1.  **Configuraci√≥n de Entorno:**
    * Instalar `gcloud CLI`, `terraform`, y `python v3.x`.
    * Autenticarse con `gcloud auth application-default login`.
    * Configurar el proyecto por defecto: `gcloud config set project medicus-data-dataml-dev`
2.  **Despliegue de Infraestructura (Terraform):**
    ```bash
    cd terraform
    terraform init
    terraform plan -out=plan.tfplan
    terraform apply plan.tfplan
    ```
3.  **Despliegue de DAGs (Composer):**
    * Subir el contenido de la carpeta `dags/` al bucket de Composer.
4.  **Ejecuci√≥n:**
    * Activar el DAG principal (`bronze_ingest_dag`) desde la UI de Airflow.
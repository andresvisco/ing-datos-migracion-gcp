# üè• Proyecto MEDICUS: Migraci√≥n Ingenier√≠a de Datos GCP

Este repositorio contiene el c√≥digo fuente, la configuraci√≥n y la documentaci√≥n para la implementaci√≥n de la plataforma de datos del proyecto **MEDICUS**, siguiendo una arquitectura de **Data Lakehouse** basada en el modelo de zonas **Bronze-Silver-Gold** sobre Google Cloud Platform (**GCP**).

El objetivo es ingestar, procesar y transformar datos de diversas fuentes de MEDICUS, asegurando calidad, trazabilidad y gobernanza para impulsar la anal√≠tica avanzada y la toma de decisiones.

---

## üó∫Ô∏è Arquitectura de Referencia

La arquitectura se basa en el **patr√≥n Medallion** (Bronze, Silver, Gold) y est√° orquestada por **Composer (Apache Airflow)**, garantizando un flujo de datos robusto, escalable y mantenible.


---

## üõ†Ô∏è Tecnolog√≠as Clave (Stack Tecnol√≥gico)

El proyecto se despliega y opera exclusivamente en **Google Cloud Platform (GCP)**, aprovechando servicios gestionados para optimizar la eficiencia operativa.

| Categor√≠a | Servicio / Tecnolog√≠a | Prop√≥sito Principal |
| :--- | :--- | :--- |
| **Orquestaci√≥n** | **Composer** (Apache Airflow) | Gesti√≥n, scheduling y monitoreo de los Data Pipelines (DAGs). |
| **Ingesta / RAW** | **Cloud Storage** (Bronze Zone) | Almacenamiento de datos crudos (RAW) en formato **Parquet** (est√°ndar Bronze). |
| **Procesamiento** | **Dataflow** / **Spark** | Transformaci√≥n de datos ETL/ELT, limpieza y enriquecimiento. |
| **Data Warehouse** | **BigQuery** (Silver & Gold Zones) | Almacenamiento optimizado para el consumo, anal√≠tica y reporting. |
| **Calidad de Datos**| **Great Expectations** | Implementaci√≥n de validaciones de calidad de datos en la zona Bronze. |
| **Metadatos/Cat√°logo** | **Data Catalog** | Descubrimiento, inventario, linaje y gesti√≥n de metadatos. |
| **Gobernanza** | **IAM, Cloud Logging, Versiones** | Seguridad, auditor√≠a, monitoreo y control de versiones de datos. |

---

## üìÇ Estructura del Repositorio

La estructura de carpetas sigue las mejores pr√°cticas para un proyecto de Data Engineering y refleja las etapas del flujo de datos:
```plaintext
medicus-data-platform/
‚îú‚îÄ‚îÄ dags/                          # Definiciones de pipelines para Composer (Airflow)
‚îÇ   ‚îú‚îÄ‚îÄ bronze_ingest_dag.py
‚îÇ   ‚îú‚îÄ‚îÄ silver_transform_dag.py
‚îÇ   ‚îî‚îÄ‚îÄ gold_analytics_dag.py
‚îú‚îÄ‚îÄ dataflow_jobs/                 # Scripts de transformaci√≥n para Dataflow
‚îÇ   ‚îú‚îÄ‚îÄ bronze_validation.py
‚îÇ   ‚îú‚îÄ‚îÄ silver_clean.py
‚îÇ   ‚îî‚îÄ‚îÄ gold_aggregation.py
‚îú‚îÄ‚îÄ sql/                           # Consultas y scripts DDL/DML para BigQuery
‚îÇ   ‚îú‚îÄ‚îÄ silver_schemas/
‚îÇ   ‚îî‚îÄ‚îÄ gold_views/
‚îú‚îÄ‚îÄ terraform/                     # Configuraci√≥n de infraestructura como c√≥digo (IaC)
‚îÇ   ‚îî‚îÄ‚îÄ gcp_resources.tf
‚îú‚îÄ‚îÄ tests/                         # Pruebas unitarias, de integraci√≥n y Great Expectations
‚îÇ   ‚îî‚îÄ‚îÄ ge_expectations.json
‚îú‚îÄ‚îÄ docs/                          # Documentaci√≥n del proyecto (diagramas, decisiones de dise√±o)
‚îî‚îÄ‚îÄ README.md            # Documentaci√≥n principal del proyecto
```

---

## üì¶ Fases del Flujo de Datos (Modelo Medallion)

### ü•â Bronze Raw Landing Zone

* **Funci√≥n:** Ingesta de datos crudos *tal cual* (RAW) desde las fuentes (`MEDICUS Source`).
* **Proceso Clave:** **Dataflow** escribe el *raw data* en Cloud Storage en formato **Parquet** (est√°ndar para capa Bronze).
* **Calidad:** Se aplica **`Great Expectations`** para validar la *integridad b√°sica* (ej. esquema, no-nulos) *antes* del procesamiento.
* **Salida:** Datos crudos y validados en **Cloud Storage (GCS)** en formato **Parquet**.
* **Ejemplo Dev:** `gs://medicus-data-bronze-raw-dev-uscentral1/raw/*.parquet`

### ü•à Silver Procesado Curado

* **Funci√≥n:** Limpieza, enriquecimiento y aplicaci√≥n de l√≥gica de negocio inicial.
* **Proceso Clave:** **Dataflow** realiza la **Transformaci√≥n** (ETL/ELT) para curar y homogeneizar los datos, leyendo archivos **Parquet** desde Bronze.
* **Salida:** Tablas limpias y curadas en **BigQuery** (`Tablas silver`).
* **Calidad:** Aplicaci√≥n de chequeos de `Calidad Curado` post-transformaci√≥n.
* **Ejemplo Dev:** Dataset `medicus-data-dataml-dev.medicus_bronze_raw_acumulado`

### ü•á Gold Anal√≠tica y Reporting

* **Funci√≥n:** Almacenamiento de datos listos para el consumo por usuarios de negocio, modelos de ML y herramientas de BI.
* **Proceso Clave:** Agregaciones, *joins*, creaci√≥n de *features* y modelos de datos dimensionales/estrella en **BigQuery**.
* **Salida:** Tablas y vistas optimizadas para el rendimiento en **BigQuery** (`Tablas Gold`).
* **Cat√°logo:** Las tablas Gold son catalogadas en **Data Catalog** para su f√°cil descubrimiento.

---

## üîí Gobernanza y Control

Se han incorporado componentes de gobernanza esenciales:

* **Versionado de Datos:** Implementaci√≥n de estrategias de *versionado* a nivel de *schema* y *data* para la auditabilidad.
* **Metadatos y Linaje:** Uso de **Data Catalog** para trazar el linaje de los datos desde la fuente (Bronze) hasta el consumo (Gold).
* **Seguridad y Auditor√≠a (IAM, PII):** Aplicaci√≥n de pol√≠ticas de **IAM** (Identity and Access Management) y controles de acceso basados en roles. Monitoreo constante a trav√©s de **Cloud Logging**.
* **Logging & Monitoring:** Integrado en todas las zonas (Bronze, Silver, Gold) para asegurar la trazabilidad operativa de los pipelines.

---

## üîß Configuraci√≥n de Entorno Dev

La configuraci√≥n del entorno de desarrollo (`dev`) sigue estrictamente la gu√≠a de nomenclatura (ver `Documentacion/nomenclatura-gcp.md`):

| Recurso | Valor Dev | Descripci√≥n |
|---------|-----------|-------------|
| **PROJECT_ID** | `medicus-data-dataml-dev` | Proyecto GCP para entorno dev |
| **BUCKET_BRONZE** | `medicus-data-bronze-raw-dev-uscentral1` | Bucket para datos raw en formato Parquet |
| **BigQuery Instance** | `medicus-data-dataml-dev` | Instancia de BigQuery para dev |
| **Dataset Bronze** | `medicus-data-dataml-dev.medicus_bronze_raw_acumulado` | Dataset para datos acumulados Bronze |
| **Formato Bronze** | **Parquet** | Formato est√°ndar para archivos en capa Bronze |
| **Regi√≥n** | `us-central1` | Regi√≥n principal para recursos dev |

> **Nota importante:** Los archivos en la capa Bronze se almacenan en formato **Parquet**, no CSV. Todos los pipelines de lectura deben usar `ReadFromParquet` en Apache Beam.

---

## ‚ñ∂Ô∏è Gu√≠a R√°pida de Inicio

1.  **Configuraci√≥n de Entorno:**
    * Instalar `gcloud CLI`, `terraform`, y `python v3.x`.
    * Autenticarse con `gcloud auth application-default login`.
2.  **Despliegue de Infraestructura (Terraform):**
    ```bash
    cd terraform
    terraform init
    terraform plan -out=plan.tfplan
    terraform apply plan.tfplan
    ```
3.  **Despliegue de DAGs (Composer):**
    * Subir el contenido de la carpeta `dags/` al bucket de Composer.
4.  **Ejecuci√≥n:**
    * Activar el DAG principal (`bronze_ingest_dag`) desde la UI de Airflow.